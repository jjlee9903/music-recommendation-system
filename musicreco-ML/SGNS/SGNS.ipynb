{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863b6f7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b084c9b6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "from IPython.display import clear_output, display\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "\n",
    "class MelonMusicRecommender:\n",
    "    def __init__(self, data_path='/content/drive/MyDrive/Graduation_Project/Dataset/'):\n",
    "        self.data_path = data_path\n",
    "        self.song_meta = None\n",
    "        self.train_data = None\n",
    "        self.val_data = None\n",
    "        self.test_data = None\n",
    "        self.song_embeddings = None\n",
    "        self.song_to_idx = {}\n",
    "        self.idx_to_song = {}\n",
    "        #self.idx_to_song_list = []\n",
    "        self.vocabulary = set()\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "        self.genre_map = {}\n",
    "        self.word_counts = None\n",
    "        self.neg_sampling_p = None\n",
    "        self.neg_sampling_cdf = None\n",
    "        self.rng = np.random.default_rng()\n",
    "\n",
    "        # 재현성 일관화를 위한 RNG/seed 관리\n",
    "        self.seed = 42\n",
    "        self.rng = np.random.default_rng(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        tf.random.set_seed(self.seed)\n",
    "        # 전처리 이후 살아남은 곡 카탈로그\n",
    "        self.kept_song_ids = set()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = re.sub(r'\\s*\\([^)]*\\)\\s*', '', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def load_data(self):\n",
    "        print(\"Loading dataset...\")\n",
    "\n",
    "        with open(os.path.join(self.data_path, 'song_meta.json'), 'r', encoding='utf-8') as f:\n",
    "            song_meta_list = json.load(f)\n",
    "\n",
    "        self.song_meta = {}\n",
    "        for song in song_meta_list:\n",
    "            self.song_meta[str(song['id'])] = song\n",
    "\n",
    "        with open(os.path.join(self.data_path, 'train.json'), 'r', encoding='utf-8') as f:\n",
    "            self.train_data = json.load(f)\n",
    "        with open(os.path.join(self.data_path, 'val.json'), 'r', encoding='utf-8') as f:\n",
    "            self.val_data = json.load(f)\n",
    "        with open(os.path.join(self.data_path, 'test.json'), 'r', encoding='utf-8') as f:\n",
    "            self.test_data = json.load(f)\n",
    "\n",
    "        genre_file_path = os.path.join(self.data_path, 'genre_gn_all.json')\n",
    "        try:\n",
    "            with open(genre_file_path, 'r', encoding='utf-8') as f:\n",
    "                self.genre_map = json.load(f)\n",
    "            print(f\"Loaded {len(self.genre_map)} genre mappings.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: The file {genre_file_path} was not found.\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON from '{genre_file_path}': {e}\")\n",
    "\n",
    "        print(f\"Loaded {len(self.song_meta)} songs\")\n",
    "        print(f\"Train playlists: {len(self.train_data)}\")\n",
    "        print(f\"Val playlists: {len(self.val_data)}\")\n",
    "        print(f\"Test playlists: {len(self.test_data)}\")\n",
    "\n",
    "    \"\"\"\n",
    "    태그 전용 전처리 (최종본)\n",
    "    규칙:\n",
    "      0) '_' 포함 태그 드롭\n",
    "      1) 중복 태그는 1회만 유지\n",
    "      2) 끝 글자가 '한' 또는 '인'이면 제거 (ex. 몽환적인 -> 몽환적)\n",
    "         + '적인' -> '적' 축약\n",
    "      3) 포함 관계: 기존 태그/새 태그가 서로 포함이면 더 짧은 쪽만 유지(접두/접미 기준)\n",
    "      4) 연도 규격화: 1990, 1990년도, 90s, '90 -> 1990 / '80-'90 -> 1980, 1990\n",
    "      5) 곡당 태그가 30개 초과면 상위 30개만, 태그 수가 3개 이하인 곡은 drop\n",
    "    추가:\n",
    "      - alias/typo 통합 (카페/까페, 캐럴/캐롤, Pop/팝 등)\n",
    "      - 방송/브랜드성 약신호 드롭\n",
    "      - vocab/네거티브 샘플링 분포 안정화\n",
    "      - 전처리 통계 및 샘플 100곡 출력\n",
    "    \"\"\"\n",
    "    def preprocess_data(self, topk_per_song: int = 30, min_tags_per_song: int = 4):\n",
    "      import re\n",
    "      import numpy as np\n",
    "      from collections import Counter, defaultdict\n",
    "\n",
    "      print(\"Preprocessing data (tags only)...\")\n",
    "\n",
    "      # ---------- 정규화 사전 ----------\n",
    "      ALIAS = {\n",
    "          \"pop\": \"팝\",\n",
    "          \"jpop\": \"제이팝\",\n",
    "          \"kpop\": \"케이팝\",\n",
    "          \"jazz\": \"재즈\",\n",
    "          \"hiphop\": \"힙합\",\n",
    "          \"rnb\": \"알앤비\",\n",
    "          \"r&b\": \"알앤비\",\n",
    "          \"soul\": \"소울\",\n",
    "          \"cafe\": \"카페\",\n",
    "          \"bgm\": \"배경음악\",\n",
    "          \"rock\": \"락\",\n",
    "          \"ost\": \"OST\",\n",
    "          \"christmas\": \"크리스마스\",\n",
    "          \"carol\": \"캐롤\",\n",
    "          \"xmas\": \"크리스마스\",\n",
    "          \"fall\": \"가을\",\n",
    "          \"moon\": \"달\"\n",
    "      }\n",
    "      TYPO = {\n",
    "          \"까페\": \"카페\",\n",
    "          \"캐럴\": \"캐롤\",\n",
    "          \"따듯\": \"따뜻\",\n",
    "          \"알엔비\": \"알앤비\",\n",
    "          \"pop\": \"팝\",\n",
    "          \"jazz\": \"재즈\",\n",
    "          \"hiphop\": \"힙합\",\n",
    "          \"jpop\": \"제이팝\",\n",
    "          \"jpop.\": \"제이팝\",\n",
    "          \"rnb\": \"알앤비\",\n",
    "          \"rnb.\": \"알앤비\",\n",
    "          \"r&b\": \"알앤비\",\n",
    "          \"r n b\": \"알앤비\",\n",
    "          \"rn b\": \"알앤비\",\n",
    "          \"rn'b\": \"알앤비\",\n",
    "          \"hip hop\": \"힙합\",\n",
    "          \"j-pop\": \"제이팝\",\n",
    "          \"k-pop\": \"케이팝\",\n",
    "          \"cafe\": \"카페\",\n",
    "          \"bgm\": \"배경음악\",\n",
    "          \"j-pop.\": \"제이팝\",\n",
    "          \"j-pop,\": \"제이팝\",\n",
    "      }\n",
    "      STOP_SINGLE = {\"r\", \"와\", \"라\"}  # 의미 약한 단일 토큰 제거\n",
    "      STOP_WEAK = {\n",
    "          \"추천\",\"인기\",\"명곡\",\"애창곡\",\"띵곡\",\"차트\",\"장르불문\",\"좋은\",\"기분\",\"노래\",\"음악\",\n",
    "          \"카카오톡\",  # 의미 없는 토큰 제거\n",
    "      }\n",
    "      BRANDS = {\"mbc\", \"jtbc\", \"fm4u\", \"오픈채팅\", \"차트100\"}  # 약신호 드롭\n",
    "\n",
    "      # ---------- 정규표현식 ----------\n",
    "      # '90, 1990, 1990년도, 1990s, '90s\n",
    "      YEAR_SINGLE_PAT = re.compile(r\"^'?(\\d{2}|\\d{4})(?:년|년도|s)?$\")\n",
    "      # '80-'90, '80~'90\n",
    "      YEAR_RANGE_PAT  = re.compile(r\"^'(\\d{2})\\s*[-~]\\s*'(\\d{2})$\")\n",
    "      # 1990년대, 90s, '90s\n",
    "      DECADE_PAT      = re.compile(r\"^'?(\\d{2}|\\d{4})\\s*(?:년대|s)$\")\n",
    "\n",
    "      def normalize_year_token(tok: str):\n",
    "          \"\"\"\n",
    "          연도/연대 규격화. 반환: 리스트(0,1,2개)\n",
    "          - '90 -> 1990  (두 자리 연도는 1900대로 해석)\n",
    "          - 1990s, 1990년대 -> 1990\n",
    "          - '80-'90 -> 1980, 1990\n",
    "          \"\"\"\n",
    "          t = tok.strip().lower()\n",
    "\n",
    "          # 범위형 '80-'90\n",
    "          mrange = YEAR_RANGE_PAT.match(t)\n",
    "          if mrange:\n",
    "              a, b = mrange.groups()\n",
    "              ya, yb = 1900 + int(a), 1900 + int(b)\n",
    "              return [str(ya), str(yb)]\n",
    "\n",
    "          # 단일 연대(년대/s)\n",
    "          mdec = DECADE_PAT.match(t)\n",
    "          if mdec:\n",
    "              g = mdec.group(1)\n",
    "              y = 1900 + int(g) if len(g) == 2 else int(g)\n",
    "              return [str(y)]\n",
    "\n",
    "          # 단일 연도\n",
    "          m = YEAR_SINGLE_PAT.match(t)\n",
    "          if m:\n",
    "              g = m.group(1)\n",
    "              if len(g) == 2:\n",
    "                  return [str(1900 + int(g))]\n",
    "              elif len(g) == 4:\n",
    "                  return [g]\n",
    "          return []\n",
    "\n",
    "      def normalize_suffix_ko(tok: str):\n",
    "          \"\"\"\n",
    "          한국어 접미 간단 정규화:\n",
    "          - '적인' -> '적'\n",
    "          - 끝 '한' 또는 '인' 제거 (예: 몽환적인 -> 몽환적, 포근한 -> 포근)\n",
    "          \"\"\"\n",
    "          t = tok\n",
    "          if t.endswith(\"적인\") and len(t) >= 3:\n",
    "              t = t[:-2]  # '적인' -> '적'\n",
    "          if t.endswith(\"한\") and len(t) > 1:\n",
    "              t = t[:-1]\n",
    "          elif t.endswith(\"인\") and len(t) > 1:\n",
    "              t = t[:-1]\n",
    "          return t\n",
    "\n",
    "      def finalize_token(p: str):\n",
    "          \"\"\"\n",
    "          최종 토큰 정리: lower, typo→alias→brand/stop 제거, 연대 규격화 검사 등\n",
    "          \"\"\"\n",
    "          t = p.strip()\n",
    "          if not t:\n",
    "              return []\n",
    "\n",
    "          # 0) '_' 포함 태그 드롭\n",
    "          if \"_\" in t:\n",
    "              return []\n",
    "\n",
    "          # 라틴계는 소문자화\n",
    "          t_low = t.lower()\n",
    "\n",
    "          # typo/alias 정규화\n",
    "          t_low = TYPO.get(t_low, t_low)\n",
    "          t_low = ALIAS.get(t_low, t_low)\n",
    "\n",
    "          # 브랜드/방송/약신호 드롭\n",
    "          if t_low in BRANDS:\n",
    "              return []\n",
    "\n",
    "          # 너무 짧은 한 글자(숫자 제외) 제거\n",
    "          if len(t_low) == 1 and not t_low.isdigit():\n",
    "              if t_low in STOP_SINGLE:\n",
    "                  return []\n",
    "              # 숫자 1자리(연도 아님)는 신호 약하므로 제거\n",
    "              return []\n",
    "\n",
    "          # 연/연대 규격화\n",
    "          years = normalize_year_token(t_low)\n",
    "          if years:\n",
    "              return years\n",
    "\n",
    "          # 한국어 접미 정규화\n",
    "          t2 = normalize_suffix_ko(t_low)\n",
    "\n",
    "          # 다시 '_' 검사\n",
    "          if \"_\" in t2 or not t2:\n",
    "              return []\n",
    "\n",
    "          # 불용어 처리\n",
    "          if t2 in BRANDS or t2 in STOP_WEAK:\n",
    "              return []\n",
    "\n",
    "          return [t2]\n",
    "\n",
    "\n",
    "\n",
    "      def tokenize_tag(raw: str):\n",
    "          \"\"\"\n",
    "          원시 태그 -> 다중 구분자 분리 -> 각 토큰 finalize\n",
    "          \"\"\"\n",
    "          txt = self.clean_text(raw) if hasattr(self, \"clean_text\") else str(raw)\n",
    "          parts = re.split(r\"[\\/,\\s#]+\", txt)\n",
    "          out = []\n",
    "          for p in parts:\n",
    "              if not p:\n",
    "                  continue\n",
    "              out.extend(finalize_token(p))\n",
    "          return out\n",
    "\n",
    "      # ---------- 1단계: 곡별 태그 카운트 집계 (플레이리스트 단위 중복 1회) ----------\n",
    "      song_tag_counter = defaultdict(Counter)  # song_id(str) -> Counter(tag->count)\n",
    "\n",
    "      for pl in self.train_data:\n",
    "          raw_tags = pl.get(\"tags\", [])\n",
    "          tokens = []\n",
    "          for t in raw_tags:\n",
    "              tokens.extend(tokenize_tag(t))\n",
    "\n",
    "          # (규칙 1) 플레이리스트 안 동일 태그 중복 1회\n",
    "          tokens = list(dict.fromkeys(tokens))\n",
    "\n",
    "          for sid in pl.get(\"songs\", []):\n",
    "              sid = str(sid)\n",
    "              song_tag_counter[sid].update(tokens)\n",
    "\n",
    "      # ---------- 포함 관계 판단 함수 (접두/접미 기준만 허용) ----------\n",
    "      def contained(shorter: str, longer: str) -> bool:\n",
    "          if len(shorter) >= len(longer):\n",
    "              return False\n",
    "          return longer.startswith(shorter) or longer.endswith(shorter)\n",
    "\n",
    "      # ---------- 포함 규칙 적용 + topK 제한 + min 필터 ----------\n",
    "      def compact_by_containment(counter: Counter) -> Counter:\n",
    "          \"\"\"\n",
    "          출현 빈도 내림차순, 길이 오름차순으로 훑으며\n",
    "          포함(접두/접미) 관계면 더 짧은 토큰만 유지.\n",
    "          \"\"\"\n",
    "          items = list(counter.items())\n",
    "          items.sort(key=lambda x: (-x[1], len(x[0]), x[0]))\n",
    "\n",
    "          kept = []  # [(tag, count)]\n",
    "          for tag, cnt in items:\n",
    "              keep_tag = tag\n",
    "              replaced = False\n",
    "              for i, (ex_tag, ex_cnt) in enumerate(kept):\n",
    "                  # ex_tag ⊂ keep_tag (ex_tag가 더 짧고 접두/접미로 포함)\n",
    "                  if contained(ex_tag, keep_tag):\n",
    "                      # 새 태그는 버리고 기존 짧은 태그 유지\n",
    "                      replaced = True\n",
    "                      break\n",
    "                  # keep_tag ⊂ ex_tag (keep_tag가 더 짧음)\n",
    "                  if contained(keep_tag, ex_tag):\n",
    "                      # 기존 긴 태그를 짧은 걸로 교체, 빈도 합산(보수적으로)\n",
    "                      kept[i] = (keep_tag, ex_cnt + cnt)\n",
    "                      replaced = True\n",
    "                      break\n",
    "              if not replaced:\n",
    "                  kept.append((keep_tag, cnt))\n",
    "          return Counter(dict(kept))\n",
    "\n",
    "      filtered_song_tags = {}\n",
    "      for sid, ctr in song_tag_counter.items():\n",
    "          if not ctr:\n",
    "              continue\n",
    "\n",
    "          # (규칙 3) 포함관계 정리\n",
    "          ctr2 = compact_by_containment(ctr)\n",
    "\n",
    "          # (규칙 5 전반부) 상위 topk 유지 (빈도 기준)\n",
    "          top_items = ctr2.most_common(topk_per_song)\n",
    "          tags = [t for t, _ in top_items]\n",
    "\n",
    "          # (규칙 5 후반부) 태그 수가 3개 이하라면 drop (min_tags_per_song=4)\n",
    "          if len(tags) < min_tags_per_song:\n",
    "              continue\n",
    "\n",
    "          filtered_song_tags[sid] = tags\n",
    "\n",
    "      # ---------- 통계/샘플 출력 ----------\n",
    "      kept_songs = len(filtered_song_tags)\n",
    "      tag_lens = [len(v) for v in filtered_song_tags.values()]\n",
    "      mean_len = float(np.mean(tag_lens)) if tag_lens else 0.0\n",
    "      median_len = float(np.median(tag_lens)) if tag_lens else 0.0\n",
    "\n",
    "      print(f\"[RESULT] kept songs: {kept_songs}\")\n",
    "      print(f\"[RESULT] avg #tags per song: {mean_len:.2f}\")\n",
    "      print(f\"[RESULT] median #tags per song: {median_len:.0f}\")\n",
    "      print(\"[SAMPLE 100 songs' tags]\")\n",
    "      for i, (sid, tags) in enumerate(filtered_song_tags.items()):\n",
    "          print(f\"{sid}: {tags}\")\n",
    "          if i >= 99:\n",
    "              break\n",
    "\n",
    "      # ---------- 학습 파이프라인 연동 ----------\n",
    "      # 문장/어휘/인덱스/네거티브 샘플링 분포 구성\n",
    "      self.song_sentences = filtered_song_tags\n",
    "      self.vocabulary = set(t for tags in filtered_song_tags.values() for t in tags)\n",
    "\n",
    "      if not self.vocabulary:\n",
    "          print(\"Warning: Vocabulary is empty after filtering.\")\n",
    "          return\n",
    "\n",
    "      global_counter = Counter(t for tags in filtered_song_tags.values() for t in tags)\n",
    "      vocab_sorted = sorted(self.vocabulary, key=lambda x: (-global_counter[x], x))\n",
    "      self.word_to_idx = {w: i for i, w in enumerate(vocab_sorted)}\n",
    "      self.idx_to_word = {i: w for w, i in self.word_to_idx.items()}\n",
    "\n",
    "      unique_songs = list(filtered_song_tags.keys())\n",
    "      self.song_to_idx = {s: i for i, s in enumerate(unique_songs)}\n",
    "      self.idx_to_song = {i: s for s, i in self.song_to_idx.items()}\n",
    "\n",
    "      # 전처리 살아남은 곡 카탈로그 저장\n",
    "      self.kept_song_ids = set(unique_songs)\n",
    "\n",
    "      # 네거티브 샘플링 분포 (0.75)\n",
    "      unigrams = np.array([global_counter[w] for w in vocab_sorted], dtype=np.float64)\n",
    "      p = np.power(unigrams, 0.75); p /= p.sum()\n",
    "      self.neg_sampling_p = p\n",
    "      self.neg_sampling_cdf = np.cumsum(p) / np.sum(p)\n",
    "\n",
    "      print(f\"Vocabulary size: {len(self.vocabulary)}\")\n",
    "      print(f\"Total songs after filtering: {len(self.song_sentences)}\")\n",
    "      any_sid = next(iter(self.song_sentences)) if self.song_sentences else None\n",
    "      if any_sid:\n",
    "          print(f\"Sample sentence for {any_sid}: {self.song_sentences[any_sid][:10]}\")\n",
    "      print(\"Negative sampling distribution and CDF created.\")\n",
    "\n",
    "\n",
    "    def build_sgns_model(self, embedding_size=128, learning_rate=0.1):\n",
    "        print(\"Building SGNS model...\")\n",
    "        vocab_size = len(self.vocabulary)\n",
    "\n",
    "        class SkipGram(keras.Model):\n",
    "            def __init__(self, vocab_size, embedding_size):\n",
    "                super().__init__()\n",
    "                self.target_embedding = layers.Embedding(vocab_size, embedding_size, name=\"target_embedding\")\n",
    "                self.context_embedding = layers.Embedding(vocab_size, embedding_size, name=\"context_embedding\")\n",
    "\n",
    "            def call(self, inputs):\n",
    "                target, context = inputs\n",
    "                target  = tf.cast(target,  tf.int32)\n",
    "                context = tf.cast(context, tf.int32)\n",
    "                t = self.target_embedding(target)   # (B, D)\n",
    "                c = self.context_embedding(context) # (B, D)\n",
    "                logits = tf.reduce_sum(t * c, axis=1)  # (B,)\n",
    "                return tf.cast(logits, tf.float32)\n",
    "\n",
    "        self.model = SkipGram(vocab_size, embedding_size)\n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, nesterov = True),\n",
    "            loss=keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        )\n",
    "        return self.model\n",
    "\n",
    "\n",
    "    def generate_training_data(self, window_size=5, num_negative_samples=10, max_samples=1000000):\n",
    "        import random\n",
    "        print(\"Generating training data...\")\n",
    "\n",
    "        positive_pairs = []\n",
    "        song_items = list(self.song_sentences.items())\n",
    "\n",
    "        for song_id, sentence in tqdm(song_items, desc=\"Creating positive samples\"):\n",
    "            if not sentence:\n",
    "                continue\n",
    "\n",
    "            word_indices = [self.word_to_idx[word] for word in sentence if word in self.word_to_idx]\n",
    "            if not word_indices:\n",
    "                continue\n",
    "\n",
    "            for i, target in enumerate(word_indices):\n",
    "                start = max(0, i - window_size)\n",
    "                end = min(len(word_indices), i + window_size + 1)\n",
    "                for j in range(start, end):\n",
    "                    if i != j:\n",
    "                        positive_pairs.append((target, word_indices[j], 1))\n",
    "                        if len(positive_pairs) >= max_samples:\n",
    "                            break\n",
    "                if len(positive_pairs) >= max_samples:\n",
    "                    break\n",
    "            if len(positive_pairs) >= max_samples:\n",
    "                break\n",
    "\n",
    "        if not positive_pairs:\n",
    "            print(\"Warning: No positive pairs created. Check data format.\")\n",
    "            return np.array([]), np.array([]), np.array([])\n",
    "\n",
    "        print(f\"Created {len(positive_pairs)} positive pairs\")\n",
    "\n",
    "        negative_pairs = []\n",
    "        vocab_size = len(self.vocabulary)\n",
    "        if self.neg_sampling_cdf is None or len(self.neg_sampling_cdf) != vocab_size:\n",
    "            print(\"Error: Negative sampling CDF is not correctly initialized.\")\n",
    "            return np.array([]), np.array([]), np.array([])\n",
    "\n",
    "        num_neg_total = len(positive_pairs) * num_negative_samples\n",
    "        num_neg_total = min(num_neg_total, max_samples * num_negative_samples)\n",
    "        print(f\"Creating {num_neg_total} negative samples using vectorized approach...\")\n",
    "\n",
    "        batch_size = 10000\n",
    "        for i in tqdm(range(0, num_neg_total, batch_size), desc=\"Creating negative samples\"):\n",
    "            batch_end = min(i + batch_size, num_neg_total)\n",
    "            batch_size_actual = batch_end - i\n",
    "            selected_indices = self.rng.integers(0, len(positive_pairs), size=batch_size_actual)\n",
    "            batch_targets = np.array([positive_pairs[idx][0] for idx in selected_indices], dtype=np.int32)\n",
    "\n",
    "            u = self.rng.random(batch_size_actual)\n",
    "            negs = np.searchsorted(self.neg_sampling_cdf, u, side=\"right\").astype(np.int32)\n",
    "\n",
    "            mask = (negs == batch_targets)\n",
    "            while mask.any():\n",
    "                u = self.rng.random(mask.sum())\n",
    "                negs[mask] = np.searchsorted(self.neg_sampling_cdf, u, side=\"right\").astype(np.int32)\n",
    "                mask = (negs == batch_targets)\n",
    "\n",
    "            negative_pairs.extend(map(tuple, np.stack(\n",
    "                [batch_targets, negs, np.zeros_like(batch_targets, dtype=np.float32)], axis=1)))\n",
    "\n",
    "        print(f\"Created {len(negative_pairs)} negative pairs\")\n",
    "\n",
    "        all_pairs = positive_pairs + negative_pairs\n",
    "\n",
    "        #np.random.shuffle(all_pairs)\n",
    "        idx_perm = self.rng.permutation(len(all_pairs))\n",
    "        all_pairs = [all_pairs[i] for i in idx_perm]\n",
    "\n",
    "        if len(all_pairs) > max_samples * (1 + num_negative_samples):\n",
    "            all_pairs = all_pairs[:max_samples * (1 + num_negative_samples)]\n",
    "\n",
    "        targets = np.array([p[0] for p in all_pairs], dtype=np.int32)\n",
    "        contexts = np.array([p[1] for p in all_pairs], dtype=np.int32)\n",
    "        labels = np.array([p[2] for p in all_pairs], dtype=np.float32)\n",
    "\n",
    "        print(f\"Total training samples: {len(targets)}\")\n",
    "        print(f\"Positive ratio: {np.mean(labels):.2%}\")\n",
    "\n",
    "        return targets, contexts, labels\n",
    "\n",
    "    def train_model(self, epochs=15, batch_size=4096, quick_mode=False):\n",
    "        \"\"\"\n",
    "        tf.data 파이프라인으로 GPU 학습\n",
    "        \"\"\"\n",
    "        print(\"Training SGNS model with tf.data...\")\n",
    "\n",
    "        targets, contexts, labels = self.generate_training_data(\n",
    "            window_size=5,\n",
    "            num_negative_samples=10,\n",
    "            max_samples=1000000\n",
    "        )\n",
    "        if len(targets) == 0:\n",
    "            print(\"Error: No training data generated. Please check your data format.\")\n",
    "            return None\n",
    "\n",
    "        targets  = targets.astype(np.int32)\n",
    "        contexts = contexts.astype(np.int32)\n",
    "        labels   = labels.astype(np.float32)\n",
    "\n",
    "        n = len(labels)\n",
    "        #idx = np.arange(n)\n",
    "        idx = self.rng.permutation(n)\n",
    "        #np.random.shuffle(idx)\n",
    "        cut = int(n * 0.8)\n",
    "        tr, va = idx[:cut], idx[cut:]\n",
    "\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "            ((targets[tr], contexts[tr]), labels[tr])\n",
    "        ).shuffle(200_000, seed=self.seed, reshuffle_each_iteration=False).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        #).shuffle(200_000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "            ((targets[va], contexts[va]), labels[va])\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        self.build_sgns_model(embedding_size=128, learning_rate=0.1)\n",
    "\n",
    "        class VisualizationCallback(keras.callbacks.Callback):\n",
    "            def __init__(self):\n",
    "                self.losses = []\n",
    "                self.val_losses = []\n",
    "                self.epochs_list = []\n",
    "            def on_epoch_end(self, epoch, logs=None):\n",
    "                self.losses.append(logs.get('loss'))\n",
    "                self.val_losses.append(logs.get('val_loss'))\n",
    "                self.epochs_list.append(epoch + 1)\n",
    "                fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "                axes[0].plot(self.epochs_list, self.losses, 'b-', label='Train')\n",
    "                axes[0].plot(self.epochs_list, self.val_losses, 'r-', label='Val')\n",
    "                axes[0].set_title('Loss'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "                if len(self.losses) > 1:\n",
    "                    diff = np.array(self.val_losses) - np.array(self.losses)\n",
    "                    axes[1].plot(self.epochs_list, diff, 'g-')\n",
    "                    axes[1].axhline(0, color='k', ls='--', alpha=0.5)\n",
    "                    axes[1].set_title('Val - Train')\n",
    "                    axes[1].grid(True, alpha=0.3)\n",
    "                plt.tight_layout(); plt.show()\n",
    "\n",
    "        viz_callback = VisualizationCallback()\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=5, restore_best_weights=True, verbose=1\n",
    "        )\n",
    "\n",
    "        history = self.model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=epochs,\n",
    "            callbacks=[viz_callback, early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        self.plot_final_results(history)\n",
    "        self.create_song_embeddings()   # 검색 행렬 생성\n",
    "        return history\n",
    "\n",
    "    def plot_final_results(self, history):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TRAINING COMPLETED - FINAL RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "        axes[0].plot(history.history['loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "        axes[0].plot(history.history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "        axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss')\n",
    "        axes[0].set_title('Final Training History'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "        min_val_loss_epoch = np.argmin(history.history['val_loss'])\n",
    "        axes[0].scatter(min_val_loss_epoch, history.history['val_loss'][min_val_loss_epoch],\n",
    "                        color='red', s=100, zorder=5)\n",
    "        axes[0].annotate(f'Best: {history.history[\"val_loss\"][min_val_loss_epoch]:.6f}',\n",
    "                         xy=(min_val_loss_epoch, history.history['val_loss'][min_val_loss_epoch]),\n",
    "                         xytext=(10, 10), textcoords='offset points',\n",
    "                         bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7),\n",
    "                         arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "        axes[1].hist(history.history['loss'], bins=20, alpha=0.5, label='Training', color='blue')\n",
    "        axes[1].hist(history.history['val_loss'], bins=20, alpha=0.5, label='Validation', color='red')\n",
    "        axes[1].set_xlabel('Loss Value'); axes[1].set_ylabel('Frequency')\n",
    "        axes[1].set_title('Loss Distribution'); axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "        summary_text = f\"\"\"\n",
    "        Final Training Metrics:\n",
    "\n",
    "        • Total Epochs: {len(history.history['loss'])}\n",
    "        • Final Train Loss: {history.history['loss'][-1]:.6f}\n",
    "        • Final Val Loss: {history.history['val_loss'][-1]:.6f}\n",
    "        • Best Val Loss: {min(history.history['val_loss']):.6f}\n",
    "        • Best Epoch: {np.argmin(history.history['val_loss']) + 1}\n",
    "\n",
    "        • Initial Loss: {history.history['loss'][0]:.6f}\n",
    "        • Loss Reduction: {(1 - history.history['loss'][-1]/history.history['loss'][0])*100:.2f}%\n",
    "        \"\"\"\n",
    "        axes[2].text(0.1, 0.5, summary_text, fontsize=11,\n",
    "                     transform=axes[2].transAxes, verticalalignment='center',\n",
    "                     bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.3))\n",
    "        axes[2].axis('off')\n",
    "        axes[2].set_title('Training Summary')\n",
    "        plt.suptitle('SGNS Model Training - Final Results', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "    def create_song_embeddings(self):\n",
    "        \"\"\"\n",
    "        학습된 단어 임베딩을 사용하여 곡 임베딩 생성\n",
    "        \"\"\"\n",
    "        print(\"Creating song embeddings...\")\n",
    "        word_embeddings = self.model.get_layer('target_embedding').get_weights()[0]\n",
    "\n",
    "        self.song_embeddings = {}\n",
    "        for song_id, sentence in tqdm(self.song_sentences.items(), desc=\"Computing song embeddings\"):\n",
    "            if len(sentence) > 0:\n",
    "                word_indices = [self.word_to_idx[word] for word in sentence if word in self.word_to_idx]\n",
    "                if word_indices:\n",
    "                    song_embedding = np.mean(word_embeddings[word_indices], axis=0)\n",
    "                    self.song_embeddings[song_id] = song_embedding\n",
    "\n",
    "        # 추천/평가용 검색 행렬 준비 (정규화 + 매핑)\n",
    "        self.build_search_matrix()\n",
    "\n",
    "    # ---------- 추천/평가 가속을 위한 추가 메서드 ----------\n",
    "    def build_search_matrix(self):\n",
    "        \"\"\"정규화 임베딩 행렬(E_norm)과 매핑 준비\"\"\"\n",
    "        if not self.song_embeddings:\n",
    "            self.E_norm = None\n",
    "            return\n",
    "        #self.idx_to_song_list = sorted(self.song_embeddings.keys(), key=int)\n",
    "        self.idx_to_song = sorted(self.song_embeddings.keys(), key=int)\n",
    "        self.song_to_idx = {sid: i for i, sid in enumerate(self.idx_to_song)}\n",
    "        D = len(next(iter(self.song_embeddings.values())))\n",
    "        E = np.zeros((len(self.idx_to_song), D), dtype=np.float32)\n",
    "        for i, sid in enumerate(self.idx_to_song):\n",
    "            E[i] = self.song_embeddings[sid]\n",
    "        E /= (np.linalg.norm(E, axis=1, keepdims=True) + 1e-12)\n",
    "        self.E_norm = E  # (N, D)\n",
    "\n",
    "    def recommend_songs(self, playlist_songs, top_k=100):\n",
    "        \"\"\"\n",
    "        빠른 추천: 정규화 임베딩 행렬과 내적 1회로 Top-K 추출\n",
    "        \"\"\"\n",
    "        if not playlist_songs or getattr(self, \"E_norm\", None) is None:\n",
    "            return []\n",
    "        seed_idxs = [self.song_to_idx[str(s)] for s in playlist_songs if str(s) in self.song_to_idx]\n",
    "        if not seed_idxs:\n",
    "            return []\n",
    "\n",
    "        q = self.E_norm[seed_idxs].mean(axis=0, dtype=np.float32)\n",
    "        q /= (np.linalg.norm(q) + 1e-12)\n",
    "\n",
    "        sims = self.E_norm @ q  # (N,)\n",
    "        sims[np.array(seed_idxs)] = -1e-9  # 시드 제외\n",
    "\n",
    "        K = min(top_k, self.E_norm.shape[0] - len(seed_idxs))\n",
    "        if K <= 0:\n",
    "            return []\n",
    "        top_idx = np.argpartition(-sims, K)[:K]\n",
    "        top_idx = top_idx[np.argsort(-sims[top_idx])]\n",
    "\n",
    "        rec_ids = []\n",
    "        for i in top_idx:\n",
    "            sid = self.idx_to_song[i]\n",
    "            #sid = self.idx_to_song_list[i]\n",
    "            rec_ids.append(int(sid) if sid.isdigit() else sid)\n",
    "        return rec_ids\n",
    "\n",
    "    def calculate_ndcg(self, recommended, actual, k=100):\n",
    "        dcg = 0.0\n",
    "        for i, song in enumerate(recommended[:k]):\n",
    "            if song in actual:\n",
    "                dcg += 1.0 / np.log2(i + 2)\n",
    "        idcg = sum([1.0 / np.log2(i + 2) for i in range(min(len(actual), k))])\n",
    "        if idcg == 0:\n",
    "            return 0.0\n",
    "        return dcg / idcg\n",
    "\n",
    "    # ---------- Metric helpers ----------\n",
    "    def precision_at_k(self, recommended, actual, k=100):\n",
    "        if k <= 0:\n",
    "            return 0.0\n",
    "        rec_k = recommended[:k]\n",
    "        hits = len(set(rec_k) & set(actual))\n",
    "        return hits / float(k)\n",
    "\n",
    "    def recall_at_k(self, recommended, actual, k=100):\n",
    "        if not actual:\n",
    "            return 0.0\n",
    "        rec_k = recommended[:k]\n",
    "        hits = len(set(rec_k) & set(actual))\n",
    "        return hits / float(len(actual))\n",
    "\n",
    "    def hit_rate_at_k(self, recommended, actual, k=100):\n",
    "        rec_k = set(recommended[:k])\n",
    "        return 1.0 if len(rec_k & set(actual)) > 0 else 0.0\n",
    "\n",
    "    \"\"\"\n",
    "    플레이리스트의 곡 중 mask_ratio(기본 30%)를 정답으로 마스킹하고,\n",
    "    나머지 곡(70%)을 시드로 사용해 Top-K 추천 → nDCG@K를 계산.\n",
    "    - min_len: 너무 짧은 리스트는 평가 제외 (기본 40)\n",
    "    - seed: 재현성 위한 난수 시드\n",
    "    \"\"\"\n",
    "    #def evaluate(self, test_playlists=None, mask_ratio=0.3, top_k_list=(100, 300, 500), min_len=40, seed=42):\n",
    "    def evaluate(self, split='val', mask_ratio=0.3, top_k_list=(100, 300, 500), min_len_catalog=30, seed=42):\n",
    "      print(\"Evaluating model with random masking...\")\n",
    "\n",
    "      # 1) 어떤 split 사용할지\n",
    "      if isinstance(split, str):\n",
    "          if split == 'test':\n",
    "              test_playlists = self.test_data\n",
    "          elif split == 'val':\n",
    "              test_playlists = self.val_data\n",
    "          else:\n",
    "              raise ValueError(\"split must be 'val', 'test', or a list of playlists.\")\n",
    "      else:\n",
    "          # 리스트 직접 전달 가능\n",
    "          test_playlists = split\n",
    "\n",
    "      rng = np.random.default_rng(seed)\n",
    "\n",
    "      # 2) 카탈로그(전처리+임베딩 존재) 기준 집합\n",
    "      kept = set(self.song_to_idx.keys())\n",
    "\n",
    "      # 3) 카탈로그 내 곡 길이 기준으로 필터\n",
    "      def to_catalog(pl):\n",
    "          catalog_songs = [s for s in pl['songs'] if str(s) in kept]\n",
    "          return catalog_songs\n",
    "\n",
    "      playlists_catalog = []\n",
    "      for pl in test_playlists:\n",
    "          cs = to_catalog(pl)\n",
    "          if len(cs) >= min_len_catalog:\n",
    "              playlists_catalog.append(cs)\n",
    "\n",
    "      if not playlists_catalog:\n",
    "          print(f\"No playlists with >= {min_len_catalog} kept songs.\")\n",
    "          return {}\n",
    "\n",
    "      print(f\"Playlists considered: {len(playlists_catalog)} (min_len_catalog={min_len_catalog})\")\n",
    "\n",
    "      # K별 누적 지표 저장용\n",
    "      sums = {k: {\"ndcg\": 0.0, \"prec\": 0.0, \"rec\": 0.0, \"hit\": 0.0} for k in top_k_list}\n",
    "      evaluated = 0\n",
    "      max_k = max(top_k_list)\n",
    "\n",
    "      for songs in tqdm(playlists_catalog, desc=\"Evaluating (random mask)\"):\n",
    "          m = max(1, int(np.ceil(len(songs) * mask_ratio)))\n",
    "          idx = np.arange(len(songs))\n",
    "          rng.shuffle(idx)\n",
    "          answer_songs = [songs[i] for i in idx[:m]]\n",
    "          input_songs  = [songs[i] for i in idx[m:]]\n",
    "          if not input_songs:\n",
    "              continue\n",
    "\n",
    "          rec = self.recommend_songs(input_songs, top_k=max_k)\n",
    "\n",
    "          for k in top_k_list:\n",
    "              ndcg = self.calculate_ndcg(rec, answer_songs, k=k)\n",
    "              prec = self.precision_at_k(rec, answer_songs, k=k)\n",
    "              reca = self.recall_at_k(rec, answer_songs, k=k)\n",
    "              hit  = self.hit_rate_at_k(rec, answer_songs, k=k)\n",
    "\n",
    "              sums[k][\"ndcg\"] += ndcg\n",
    "              sums[k][\"prec\"] += prec\n",
    "              sums[k][\"rec\"]  += reca\n",
    "              sums[k][\"hit\"]  += hit\n",
    "\n",
    "          evaluated += 1\n",
    "\n",
    "      if evaluated == 0:\n",
    "          print(\"No playlists evaluated.\")\n",
    "          return {}\n",
    "\n",
    "      results = {}\n",
    "      for k in top_k_list:\n",
    "          results[k] = {\n",
    "              \"nDCG\":   sums[k][\"ndcg\"] / evaluated,\n",
    "              \"Prec\":   sums[k][\"prec\"] / evaluated,\n",
    "              \"Recall\": sums[k][\"rec\"]  / evaluated,\n",
    "              \"Hit\":    sums[k][\"hit\"]  / evaluated,\n",
    "          }\n",
    "\n",
    "      print(f\"\\n=== Averages (split={split}, mask={int(mask_ratio*100)}%, len≥{min_len_catalog}, evaluated={evaluated}) ===\")\n",
    "      for k in top_k_list:\n",
    "          r = results[k]\n",
    "          print(f\"K={k:>4}  nDCG={r['nDCG']:.4f}  Prec={r['Prec']:.4f}  Recall={r['Recall']:.4f}  HitRate={r['Hit']:.4f}\")\n",
    "\n",
    "      return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb59ff1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 메인 실행 코드\n",
    "print(\"=\"*60)\n",
    "print(\"MUSIC RECO MODEL\")\n",
    "print(\"Based on Data Embedding (SGNS)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "recommender = MelonMusicRecommender()\n",
    "\n",
    "print(\"\\n[Step 1/5] Loading Dataset...\")\n",
    "recommender.load_data()\n",
    "\n",
    "print(\"\\n[Step 2/5] Preprocessing Data...\")\n",
    "recommender.preprocess_data()\n",
    "\n",
    "print(\"\\n--- Sample of Preprocessed Song Sentences (20 samples) ---\")\n",
    "sample_count = 0\n",
    "for song_id, sentence in recommender.song_sentences.items():\n",
    "    print(f\"Song ID: {song_id}\")\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(\"-\" * 20)\n",
    "    sample_count += 1\n",
    "    if sample_count >= 20:\n",
    "        break\n",
    "print(\"------------------------------------------------------------\\n\")\n",
    "\n",
    "print(\"\\n[Step 3/5] Training Model...\")\n",
    "history = recommender.train_model(\n",
    "    epochs=30,\n",
    "    batch_size=4096,   # GPU면 4096~8192 권장\n",
    "    quick_mode=False\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n[Step 3-end] Saving Model...\")\n",
    "# 1) 단어(=태그) 임베딩 행렬 뽑기\n",
    "W = recommender.model.get_layer('target_embedding').get_weights()[0]  # shape: [vocab_size, 128]\n",
    "\n",
    "# 2) 인덱스↔단어 사전 가져오기\n",
    "word_to_idx = recommender.word_to_idx   # {tag -> idx}\n",
    "idx_to_word = {i:w for w,i in word_to_idx.items()}\n",
    "\n",
    "# 3) 태그 임베딩을 딕셔너리로 저장 (정규화)\n",
    "tag_embeds = {}\n",
    "for i, w in idx_to_word.items():\n",
    "    v = W[i].astype(np.float32)\n",
    "    v = v / (np.linalg.norm(v) + 1e-12)\n",
    "    tag_embeds[w] = v\n",
    "\n",
    "\n",
    "save_path = '/content/drive/MyDrive/Graduation_Project/model_checkpoint/'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(save_path, 'tag_embeddings.pkl'), 'wb') as f:\n",
    "    pickle.dump(tag_embeds, f)\n",
    "\n",
    "with open(os.path.join(save_path, 'word_to_idx.json'), 'w', encoding=\"utf-8\") as f:\n",
    "  json.dump(word_to_idx, f, ensure_ascii=False)\n",
    "\n",
    "with open(os.path.join(save_path, 'song_embeddings_adam_ver4.pkl'), 'wb') as f:\n",
    "    pickle.dump(recommender.song_embeddings, f)\n",
    "\n",
    "print(\"saved tag_embeddings.pkl & word_to_idx.json\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
